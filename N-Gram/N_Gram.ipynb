{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs, uniout, operator\n",
    "\n",
    "def cutSentence(filePath, longTerms): #建立方法(檔案路徑，關鍵字)\n",
    "        cutlist = '<>/:：;；,、＂’，.。！？「」\\\"\\'\\\\\\n\\r《》『』“”!@#$%^&*()'.decode('utf-8') #標點符號要去除\n",
    "        text = codecs.open(filePath, 'r', 'utf-8') #利用codecs處理編碼並且用utf-8讀入\n",
    "        sentence = '' #建立空字串，放入單一中文字\n",
    "        textList = [] #建立空List，會將setence的字串一一放入\n",
    "        \n",
    "        for lines in text.readlines(): #讀一行\n",
    "            lines = lines.strip() #去除空白\n",
    "            \n",
    "            for keyword in longTerms:\n",
    "                lines = ''.join(lines.split(keyword)) #去除長詞中的關鍵字，避免重複\n",
    "                \n",
    "            for word in lines: #一一讀取每一個字\n",
    "                if word not in cutlist: #如果該中文字非標點符號，則一一加進setence內\n",
    "                    sentence += word\n",
    "                else: #如果是的話，則將setence內的字串加入textList內，並且同時宣告一個空的setence字串\n",
    "                    textList.append(sentence)\n",
    "                    sentence = ''\n",
    "        \n",
    "        return textList #回傳list\n",
    "\n",
    "def ngram(textList, maxTermLength, minFreq):\n",
    "        words = [] #存放字詞\n",
    "        words_freq_dict = {} #存放'字詞':詞頻\n",
    "        result = [] #存放最後的結果\n",
    "        \n",
    "        for strList in textList: #從上一個方法中取得回傳值，並且一一取出，['字串', '字串', '字串', '字串'...]\n",
    "            for length in range(0, len(strList) - (maxTermLength - 1)): #\n",
    "                words.append(strList[length:length + maxTermLength])\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in words_freq_dict: #如果取出的中文字詞(key)不在dictionary內，則計算有該字詞的數量，並且丟入指定的key\n",
    "                words_freq_dict[word] = words.count(word)\n",
    "        \n",
    "        words_freq_list = sorted(words_freq_dict.iteritems(), key = operator.itemgetter(1), reverse = True)\n",
    "        #針對存放{'字詞':詞頻}的字典做排序，並轉換成一個list[['字詞',詞頻], ['字詞',詞頻], ['字詞',詞頻]...]\n",
    "        \n",
    "        for word in words_freq_list: #針對list中的元素，一一取出[['字詞',詞頻], ['字詞',詞頻], ['字詞',詞頻]...]\n",
    "            if word[1] >= minFreq: #如果取出來的所對應到索引值[1]的元素≧minFreq，一一將word這個list加進result內\n",
    "                result.append(word)\n",
    "        \n",
    "        return result #回傳result這個list\n",
    "\n",
    "def longTermPriority(filePath, maxTermLength, minFreq): #(檔案路徑，字詞長度，最少詞頻)\n",
    "        longTerms = [] #存放長字詞\n",
    "        longTermsFreq = [] #存放長字詞與詞頻\n",
    "        \n",
    "        for termsNum in range(maxTermLength, 1, -1): #從最大的字詞長度開始，直到1為止，每執行一次-1\n",
    "            text_list = cutSentence(filePath, longTerms) #呼叫方法\n",
    "            words_freq = ngram(text_list, termsNum, minFreq) #呼叫方法\n",
    "        \n",
    "            for word_freq in words_freq: #從回傳result的list中，一一取出元素\n",
    "                longTerms.append(word_freq[0]) #索引值[0]所對應的元素，則為下一次長字詞中，刪去重複的關鍵字詞\n",
    "                longTermsFreq.append(word_freq) #將['字詞',詞頻]，一一加入\n",
    "        \n",
    "        return longTermsFreq #回傳list\n",
    "\n",
    "def resultPrintOut(TermFreqResult):\n",
    "    print '字詞', '\\t詞頻'\n",
    "    print '============'\n",
    "    for result in TermFreqResult: #印出結果\n",
    "        print result[0], '\\t' + str(result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字詞 \t詞頻\n",
      "============\n",
      "那僧道 \t6\n",
      "士隱聽 \t6\n",
      "空道人 \t5\n",
      "隱聽了 \t5\n",
      "那道人 \t5\n",
      "空空道 \t5\n",
      "士隱 \t38\n",
      "雨村 \t24\n",
      "笑道 \t17\n",
      "那僧 \t11\n",
      "不知 \t10\n",
      "道人 \t9\n",
      "一段 \t8\n",
      "弟子 \t8\n",
      "不過 \t8\n",
      "世人 \t8\n",
      "丫鬟 \t8\n",
      "了一 \t7\n",
      "風流 \t7\n",
      "故事 \t7\n",
      "石頭 \t7\n",
      "去了 \t7\n",
      "有一 \t7\n",
      "二人 \t7\n",
      "紅塵 \t6\n",
      "意欲 \t6\n",
      "聽了 \t6\n",
      "英蓮 \t6\n",
      "富貴 \t6\n",
      "不可 \t6\n",
      "原來 \t6\n",
      "這一 \t6\n",
      "蠢物 \t6\n",
      "如此 \t6\n",
      "其中 \t5\n",
      "人都 \t5\n",
      "又有 \t5\n",
      "心中 \t5\n",
      "幾個 \t5\n",
      "之人 \t5\n",
      "之事 \t5\n",
      "明白 \t5\n",
      "說道 \t5\n",
      "有些 \t5\n",
      "不能 \t5\n",
      "也不 \t5\n",
      "自己 \t5\n",
      "封肅 \t5\n",
      "如今 \t5\n",
      "說著 \t5\n",
      "不了 \t5\n",
      "神仙 \t5\n",
      "下世 \t5\n",
      "女子 \t5\n"
     ]
    }
   ],
   "source": [
    "longTermFreq = longTermPriority('D:\\\\BigData\\\\Python\\\\Workspace\\\\JiebaCut\\\\txtFiles\\\\test.txt', 3, 5) #(檔案路徑，字詞長度，最少詞頻)\n",
    "resultPrintOut(longTermFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
